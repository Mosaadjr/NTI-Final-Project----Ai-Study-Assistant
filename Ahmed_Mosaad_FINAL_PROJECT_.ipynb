{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+ZrfkxuYpUjnefS0peh7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mosaadjr/NTI-Final-Project----Ai-Study-Assistant/blob/main/Ahmed_Mosaad_FINAL_PROJECT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***AI STUDY ASSISTANT ***"
      ],
      "metadata": {
        "id": "MQFMTmrDPp6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KmshuwWevIt",
        "outputId": "120f6bb3-7fa0-4d19-f128-e567e6ba1939"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "def extract_clean_text(pdf_path):\n",
        "    doc = fitz.open('/content/Word2Vec .pdf')\n",
        "    text = \"\"\n",
        "\n",
        "    for page in doc:\n",
        "        # Extract page text\n",
        "        page_text = page.get_text()\n",
        "        if page_text:\n",
        "            text += page_text + \"\\n\"\n",
        "\n",
        "    # --- Cleaning Step ---\n",
        "    # 1. Remove control/non-printable characters\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n]', '', text)\n",
        "\n",
        "    # Remove lines that look like garbled code (long sequences of caps/numbers/symbols)\n",
        "    text = re.sub(r'^[A-Z0-9\\/\\*\\]\\[\\\\\\']{5,}$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 2. Remove repeated single letters or symbols used as formatting (like T, }, #)\n",
        "    text = re.sub(r'\\b[T#}\\*]+\\b', '', text)\n",
        "\n",
        "    # 2. Remove long sequences of uppercase letters (likely artifacts)\n",
        "    text = re.sub(r'\\b[A-Z]{11,}\\b', '', text)\n",
        "\n",
        "    # Remove stray symbols like } { ]\n",
        "    text = re.sub(r'[}{\\[\\]\\\\]', '', text)\n",
        "\n",
        "     # 4. Remove excessive spaces\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # 3. Replace multiple newlines with a single newline\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "    # 5. Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return text\n",
        "\n",
        "# --- Example Usage ---\n",
        "pdf_text = extract_clean_text(\"/content/Word2Vec .pdf\")\n",
        "print(pdf_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULaGDR9Drle8",
        "outputId": "542867a9-592b-4512-8367-f9316daa956e",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NATURAL \n",
            " \n",
            "LECTURE AGENDA\n",
            "3\n",
            " Word2Vec\n",
            " Text \n",
            "Classification\n",
            "VEC\n",
            "4\n",
            "WHATS WORD2VEC?\n",
            "4\n",
            "Technique/Algorithm for natural language processing published in 2013.\n",
            "Represents each distinct word with a particular list of numbers called \n",
            "a vector.\n",
            "The vectors are chosen carefully such that a simple mathematical function \n",
            "(the cosine similarity between the vectors) indicates the level of semantic \n",
            "similarity between the words represented by those vectors.\n",
            "Uses a two-layer neural network model to learn word associations from a\n",
            "large corpus of text.\n",
            "The input to the NN is the text corpus and the output is vector of \n",
            "features.\n",
            "WHATS WORD2VEC?\n",
            "5\n",
            "Once trained, such a model can\n",
            "1.\n",
            "Detect synonymous words.\n",
            "2.\n",
            "Suggest additional words for a partial sentence.\n",
            "It uses word embeddings in training the model.\n",
            "Its main purpose is to compute the weight of each word in the \n",
            "sentence which indicates its rank and thus can detect the \n",
            "missing word.\n",
            "It can detect the relatedness of words such as man-boy= woman-girl.\n",
            "It can also detect the singulars and the plurals which helps in natural \n",
            "language analysis and modeling.\n",
            "When trained in a large amount of data, it can detect synonyms of \n",
            "words.\n",
            "WORD2VEC MODEL \n",
            "Two types of \n",
            "Word2Vec :\n",
            "1.\n",
            "CBOW Continuous Bag Of Words: the model predicts the current word from a window of \n",
            "surrounding\n",
            "context words.\n",
            "Example: I went to the restaurant to order .. . \n",
            "(Food)\n",
            "I went to to order food. (the \n",
            "restaurant)\n",
            "2.\n",
            "Skip-Grams: the model uses the current word to predict the surrounding window of context words. The \n",
            "skip- gram architecture weighs nearby context words more heavily than more distant context words. It \n",
            "requires text generation.\n",
            "Example: We ?\n",
            "We shall.\n",
            "We shall not .\n",
            "We shall not accept \n",
            "We shall not accept this \n",
            "deal.\n",
            "CBOW is faster while skip-gram does a better job for infrequent words.\n",
            "7\n",
            "WORD2VEC: CBOW\n",
            "7\n",
            "It is a hybrid technique using BOW and N-Gram techniques.\n",
            "BOW is the representation of each word.\n",
            "The bag-of-words model is a simplifying representation used in natural language \n",
            "processing and information retrieval (IR), where the text (such as a sentence or a document) \n",
            "is represented as the bag (multiset) of its words, disregarding grammar and even word order \n",
            "but keeping multiplicity.\n",
            "The bag-of-words model is commonly used in methods of document classification where \n",
            "the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
            "The Bag-of-words model is one example of a Vector space model.\n",
            "WORD2VEC: CBOW\n",
            "8\n",
            "Continuous Bag of Words is a hybrid model using BOW and N-Grams.\n",
            "BOW: depends on the words found in the context and represent them as 0s or 1s or \n",
            "percentage of the word existence.\n",
            "NGram: a contiguous sequence of n items from a given sample of text or speech. The items \n",
            "can be phonemes, syllables, letters, words or base pairs according to the application. The \n",
            "n-grams typically are collected from a text or speech corpus. Using Latin numerical prefixes, \n",
            "an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \n",
            "\"digram\"); size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \n",
            "\"four-gram\", \"five-gram\", and so on.\n",
            "CBOW: using more than one existing word to detect a specific word (most commonly the \n",
            "last one in the sentence), via NN utilizing the embedding matrix for the input words.\n",
            "1\n",
            "0\n",
            "Context size could be 2 or more (normally between 5 and 10) in \n",
            "both ways.\n",
            "WORD2VEC: CBOW- \n",
            "Used in Article Spinning which means to rephrase the article with \n",
            "new words/synonyms keeping the same meaning but without being \n",
            "considered as a plagiarized version.\n",
            "This is performed by first removing some words from the original \n",
            "article (randomly or with a certain criteria to be automated, i.e. \n",
            "remove every 6th wordetc), and then let the CBOW model predict \n",
            "the missing words (if the same word was predicted, re-iterate to get \n",
            "different word).\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "CBOW \n",
            "Vs.Skip-Gram\n",
            "WORD2VEC: \n",
            "SKIP-GRAM TRAINING\n",
            "Predicts a sentence given a \n",
            "word\n",
            "1\n",
            "4\n",
            "WORD2VEC: SKIP-GRAM\n",
            "It is to be noted that we should specify the maximum size \n",
            "of word relatedness.\n",
            " For example, if the maximum =4, then the window size is two-words \n",
            "surrounding the specified word.\n",
            "Then train the NN to be able to predict the upcoming word if we \n",
            "just give it 1 word as input.\n",
            "We input to the network the embedding matrix of each word and \n",
            "its one\n",
            "hot encoder. Then calculate the multiplication of the two matrices.\n",
            "The embedding matrix is created by either initializing random \n",
            "values and through training it is set or get it from a built-in library \n",
            "like SpaCy.\n",
            "1\n",
            "5\n",
            "WORD2VEC: SKIP-GRAM\n",
            "1\n",
            "6\n",
            "WORD2VEC: \n",
            "SKIP-GRAM\n",
            "featur\n",
            "e\n",
            "1\n",
            "7\n",
            "outpu\n",
            "t\n",
            "1\n",
            "8\n",
            "CB\n",
            "OW\n",
            "We will build it from scratch, no libraries used.\n",
            "Text domain (Generic, Economics.etc) and writing style are specified \n",
            "according to the domain of the trained input text.\n",
            "Re.sub(): remove everything but alphanumeric\n",
            "1\n",
            "9\n",
            "2\n",
            "0\n",
            "2\n",
            "1\n",
            "Context size: we take two- word window (2 before the word and 2 \n",
            "after it)\n",
            "Embed dim: embedding matrix size\n",
            "2\n",
            "2\n",
            " Context= feature\n",
            " Context size=2\n",
            " 1st Word Bag: starting by the word about and taking the surrounding 2 \n",
            "words prior\n",
            "and next to it as its features (we are , to study)\n",
            " 2nd Word Bag: the word to and taking the surrounding 2 words prior and \n",
            "next to it as its features (are about , study the)\n",
            " ..and so on.\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SUMMERIZATION & MCQ GENERATION ***"
      ],
      "metadata": {
        "id": "jKuPKapR3oQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# --- configure Gemini API key ---\n",
        "genai.configure(api_key=\"AIzaSyC64tmJ98n1Klel_wJyWaXSWIP2dIFatu0\")\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "#--- SUMMERIZATION---\n",
        "def summarize_text(text):\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following text in clear bullet points:\n",
        "\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "#---MCQ_GENERATION---\n",
        "def mcq_generation(text , num_questions = 5):\n",
        "    prompt = f\"\"\"\n",
        "    Generate {num_questions} multiple choice questions based on the following text:\n",
        "\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# Example usage after cleaning\n",
        "cleaned_pdf_text = extract_clean_text(\"/content/Word2Vec .pdf\")\n",
        "\n",
        "summary = summarize_text(cleaned_pdf_text)\n",
        "print(\"===== SUMMARY =====\")\n",
        "print(summary)\n",
        "\n",
        "mcqs = mcq_generation(cleaned_pdf_text)\n",
        "print(\"===== MCQ's =====\")\n",
        "print(mcqs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eE9IGrwJuu44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1e9be36-bf37-4ae8-c137-f67f19b271b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== SUMMARY =====\n",
            "* **Word2Vec:** A 2013 natural language processing technique that represents words as numerical vectors.  Cosine similarity between these vectors indicates semantic similarity.  It uses a two-layer neural network trained on a large text corpus.\n",
            "\n",
            "* **Word2Vec Applications:**  Detects synonyms, suggests words for incomplete sentences, identifies word relationships (e.g., man-boy=woman-girl), and handles singular/plural forms.\n",
            "\n",
            "* **Word2Vec Model Types:**\n",
            "    * **CBOW (Continuous Bag-of-Words):** Predicts a word based on its surrounding context words.  Faster than Skip-gram but less effective with infrequent words.  Uses a hybrid of Bag-of-Words and N-gram techniques.\n",
            "    * **Skip-gram:** Predicts surrounding context words given a word.  Weights nearby words more heavily. Better for infrequent words and requires text generation.\n",
            "\n",
            "* **Bag-of-Words (BOW):** A simplified text representation where word order is ignored, and words are represented as a multiset (bag) of their frequencies. Used as a feature in document classification.\n",
            "\n",
            "* **N-grams:** Contiguous sequences of *n* items (letters, words, etc.) from a text.  Examples include unigrams (single words), bigrams (pairs of words), trigrams (three-word sequences), etc.\n",
            "\n",
            "* **CBOW Implementation Details:** Uses an embedding matrix and can be applied to article spinning (rephrasing articles while avoiding plagiarism).\n",
            "\n",
            "* **Skip-gram Implementation Details:** Specifies a maximum word relationship size (determining the context window).  Uses an embedding matrix (either randomly initialized or from a library like SpaCy) and one-hot encoding for input.  A neural network predicts subsequent words given a single word input.\n",
            "\n",
            "\n",
            "* **CBOW vs. Skip-gram:** CBOW is faster, while Skip-gram performs better with infrequent words.  The choice depends on specific needs and dataset characteristics.  A practical example of building a Skip-gram model from scratch without using libraries is given in the text.\n",
            "\n",
            "===== MCQ's =====\n",
            "Here are 5 multiple-choice questions based on the provided text:\n",
            "\n",
            "1. **Word2Vec is best described as:**\n",
            "    a) A single-layer neural network for text classification.\n",
            "    b) A technique for representing words as vectors based on semantic similarity.\n",
            "    c) A method for directly measuring the frequency of words in a text corpus.\n",
            "    d) An algorithm solely focused on identifying grammatical errors in text.\n",
            "\n",
            "2. **What is the key difference between CBOW and Skip-gram in Word2Vec?**\n",
            "    a) CBOW predicts the context words given a target word, while Skip-gram predicts the target word given the context words.\n",
            "    b) CBOW uses a single-layer neural network, while Skip-gram uses a multi-layer network.\n",
            "    c) CBOW is only suitable for large corpora, while Skip-gram works well with smaller datasets.\n",
            "    d) CBOW focuses on frequent words, while Skip-gram excels at identifying infrequent words.\n",
            "\n",
            "\n",
            "3. **In the context of Word2Vec's CBOW model, what does BOW represent?**\n",
            "    a)  Backpropagation Over Words\n",
            "    b)  Bag of Words\n",
            "    c)  Binary Output Weights\n",
            "    d)  Best Of Words\n",
            "\n",
            "\n",
            "4. **Which of the following is NOT a typical application of a trained Word2Vec model?**\n",
            "    a) Detecting synonyms.\n",
            "    b) Suggesting words to complete a sentence.\n",
            "    c) Identifying the grammatical role of each word in a sentence.\n",
            "    d)  Finding analogies like \"man-boy = woman-girl\".\n",
            "\n",
            "\n",
            "5. **In the Skip-gram model, what is the purpose of specifying a \"maximum size of word relatedness\"?**\n",
            "    a) It determines the size of the vocabulary.\n",
            "    b) It defines the number of hidden layers in the neural network.\n",
            "    c) It sets the size of the context window considered around the target word.\n",
            "    d) It limits the number of iterations during the training process.\n",
            "\n",
            "\n",
            "**Answer Key:**\n",
            "\n",
            "1.  b)\n",
            "2.  a)\n",
            "3.  b)\n",
            "4.  c)\n",
            "5.  c)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zl3NLQe5-WyP",
        "outputId": "2c82191b-4e2e-434f-9fcc-a4f45af88379"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=d99b35e6ea8d7e661c97fcb68af201622c334a72f0ddeb0ed9711acf5820c1d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/62/11/dc73d78e40a218ad52e7451f30166e94491be013a7850b5d75\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "def save_summary_mcq(summary,mcqs,output_file='output.pdf'):\n",
        "  pdf = FPDF()\n",
        "  pdf.set_auto_page_break(auto=True, margin=15)\n",
        "  pdf.add_page()\n",
        "  pdf.set_font('Arial', 'B', 16)\n",
        "\n",
        "  # TITLE\n",
        "  pdf.cell(0,10,'summary & mcq',ln=True,align='C')\n",
        "  pdf.ln(10)\n",
        "\n",
        "  # SUMMARY\n",
        "  pdf.set_font('Arial', 'B', 14)\n",
        "  pdf.cell(0,10,'summary',ln=True)\n",
        "  pdf.set_font('Arial', '', 12)\n",
        "  pdf.multi_cell(0,5,summary)\n",
        "  pdf.ln(10)\n",
        "\n",
        "  # MCQ\n",
        "  pdf.set_font('Arial', 'B', 14)\n",
        "  pdf.cell(0,10,'mcqs',ln=True)\n",
        "  pdf.set_font('Arial', '', 12)\n",
        "  pdf.multi_cell(0,5,mcqs)\n",
        "  pdf.ln(10)\n",
        "\n",
        "  pdf.output(output_file)\n",
        "\n",
        "save_summary_mcq(summary,mcqs)"
      ],
      "metadata": {
        "id": "eMWX7JPpGwsV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def pdf_nlp_interface(task, pdf_file):\n",
        "    if pdf_file is None:\n",
        "        return \"Please upload a PDF file.\"\n",
        "\n",
        "    cleaned_text = extract_clean_text(pdf_file.name)\n",
        "\n",
        "    if task == \"Summarize\":\n",
        "        summary = summarize_text(cleaned_text)\n",
        "        return summary\n",
        "    elif task == \"MCQs\":\n",
        "        summary = summarize_text(cleaned_text)\n",
        "        mcqs = mcq_generation(cleaned_text)\n",
        "        save_summary_mcq(summary, mcqs, \"output.pdf\")\n",
        "        return f\"{mcqs}\\n\\nPDF saved as output.pdf\"\n",
        "    else:\n",
        "        return \"Invalid task selected.\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=pdf_nlp_interface,\n",
        "    inputs=[\n",
        "        gr.Dropdown([\"Summarize\", \"MCQs\"], label=\"Select Task\"),\n",
        "        gr.File(label=\"Upload PDF\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Output\"),\n",
        "    title=\"Mini NLP PDF Assistant\",\n",
        "    description=\"Upload a PDF and either get a summary or generate MCQs (PDF downloadable).\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "su_40HzwfOD7",
        "outputId": "8e777359-266c-4907-d9b6-9b05fb380eaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b14c6a1ea07571be30.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b14c6a1ea07571be30.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "DmbIbjYfPTRk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1e9b7a33",
        "outputId": "9f28f638-b80b-419d-e3b6-fe3fedaa5420"
      },
      "source": [
        "!pip install rouge_score"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7a54260f40e7a693860e2a66e7a13820971deb93a867d3019614cb32d94f0662\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4t4vgj1uNPYI",
        "outputId": "a598f9f5-cb9e-45ac-ab11-1615766c8746"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu"
      ],
      "metadata": {
        "id": "PQRx8B8MNG-0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFRENCE FOR EVALUATION FROM stydyaitool.com its ai study assistant tool.\n",
        "\n",
        "reference_summary = \"\"\" The word2vec model is highlighted, specifically the Continuous Bag of Words (CBOW) technique, which combines bag-of-words and n-gram methods.\n",
        "The bag-of-words model is explained as a representation of text that disregards grammar and word order but retains word frequency, commonly used in document classification.\n",
        "The context size for word2vec can typically range from 5 to 10.\n",
        "The agenda includes topics on word2vec and text classification.\n",
        "\"\"\"\n",
        "\n",
        "reference_mcqs = \"\"\"\n",
        "Q1. What is the primary focus of the document regarding model construction?\n",
        "A) Using pre-built libraries for efficiency\n",
        "B) Building a model from scratch without libraries*\n",
        "C) Applying machine learning algorithms directly\n",
        "D) Focusing solely on data visualization techniques\n",
        "\n",
        "Q2. In the context of word2vec, what does CBOW stand for?\n",
        "A) Continuous Bag of Words*\n",
        "B) Cumulative Bag of Output Words\n",
        "C) Composite Bag of Ordered Words\n",
        "D) Contextual Bag of Weighted Words\n",
        "\n",
        "Q3. How does the bag-of-words model treat grammar and word order?\n",
        "A) It preserves both grammar and word order\n",
        "B) It disregards grammar but retains word order\n",
        "C) It disregards both grammar and word order*\n",
        "D) It focuses on grammar while ignoring word frequency\n",
        "\n",
        "Q4. What is the typical range for context size when implementing word2vec?\n",
        "A) 1 to 3\n",
        "B) 2 to 5\n",
        "C) 5 to 10*\n",
        "D) 10 to 15\n",
        "\n",
        "Q5. Which technique is mentioned as a hybrid method that combines bag-of-words and n-gram techniques?\n",
        "A) Skip-gram\n",
        "B) CBOW*\n",
        "C) TF-IDF\n",
        "D) LSTM\n",
        "\"\"\"\n",
        "\n",
        "generated_summary = summary\n",
        "generated_mcqs = mcqs\n",
        "\n",
        "# --- ROUGE Evaluation ---\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
        "\n",
        "# Summarization ROUGE\n",
        "summary_scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "# MCQ ROUGE\n",
        "mcq_scores = scorer.score(reference_mcqs, generated_mcqs)\n",
        "\n",
        "print(\"=== ROUGE Scores ===\")\n",
        "print(\"Summary:\", summary_scores)\n",
        "print(\"MCQs:\", mcq_scores)\n",
        "\n",
        "# --- BLEU Evaluation (using SacreBLEU) ---\n",
        "# Summarization BLEU\n",
        "bleu_summary = sacrebleu.corpus_bleu([generated_summary], [[reference_summary]])\n",
        "# MCQ BLEU\n",
        "bleu_mcq = sacrebleu.corpus_bleu([generated_mcqs], [[reference_mcqs]])\n",
        "\n",
        "print(\"\\n=== BLEU Scores ===\")\n",
        "print(\"Summary BLEU:\", bleu_summary.score)\n",
        "print(\"MCQ BLEU:\", bleu_mcq.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fxWf8RUNUli",
        "outputId": "50146b85-52cc-457c-ea20-63ca535ee269"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ROUGE Scores ===\n",
            "Summary: {'rouge1': Score(precision=0.18505338078291814, recall=0.7428571428571429, fmeasure=0.2962962962962963), 'rouge2': Score(precision=0.060714285714285714, recall=0.2463768115942029, fmeasure=0.09742120343839543), 'rougeL': Score(precision=0.11387900355871886, recall=0.45714285714285713, fmeasure=0.18233618233618232)}\n",
            "MCQs: {'rouge1': Score(precision=0.3402061855670103, recall=0.5625, fmeasure=0.423982869379015), 'rouge2': Score(precision=0.10344827586206896, recall=0.17142857142857143, fmeasure=0.12903225806451615), 'rougeL': Score(precision=0.1718213058419244, recall=0.2840909090909091, fmeasure=0.21413276231263384)}\n",
            "\n",
            "=== BLEU Scores ===\n",
            "Summary BLEU: 1.2387339768333108\n",
            "MCQ BLEU: 2.691706807498212\n"
          ]
        }
      ]
    }
  ]
}